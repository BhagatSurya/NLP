{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Your Shakespeare Text Using Sequential Models Such As Long-Short-Term-Memory (LSTMs), Gated Recurrent Units (GRUs), Recurrent Neural Network (RNNs).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQnvuxUlpQpLmYXjBnX1iW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhagatSurya/NLP/blob/main/Generating_Your_Shakespeare_Text_Using_Sequential_Models_Such_As_Long_Short_Term_Memory_(LSTMs)%2C_Gated_Recurrent_Units_(GRUs)%2C_Recurrent_Neural_Network_(RNNs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "#from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os\n",
        "\n",
        "cache_dir = './tmp'\n",
        "dataset_file_name = 'shakespeare.txt'\n",
        "dataset_file_origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n",
        "\n",
        "print(dataset_file_path) \n",
        "ss = open(dataset_file_path,mode='r')\n",
        "text = ss.read()\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "def build_data(text, Tx = 100, stride = 1):  \n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(0, len(text) - Tx, stride):\n",
        "        X.append(text[i: i + Tx])\n",
        "        Y.append(text[i + Tx]) \n",
        "    print('number of training examples:', len(X))\n",
        "    return X, Y\n",
        "\n",
        "X,Y = build_data(text[:10000])\n",
        "\n",
        "\n",
        "\n",
        "def vectorization(X, Y, n_x, char_indices, Tx = 100):\n",
        "    m = len(X)\n",
        "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
        "    y = np.zeros((m, n_x), dtype=np.bool)\n",
        "    for i, sentence in enumerate(X):\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[Y[i]]] = 1    \n",
        "    return x, y \n",
        "\n",
        "x,y = vectorization(X,Y,len(chars),char_indices,Tx=100)\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
        "    return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TM9YYIAvTga",
        "outputId": "2279aab2-e077-440b-e50b-efa069a9a93c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/.keras/datasets/shakespeare.txt\n",
            "number of training examples: 9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(100, len(chars)),return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "maxlen = 100\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "  if(epoch>0 and epoch%150 == 0):\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.5]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(500):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=512,\n",
        "          epochs=200,\n",
        "          callbacks=[print_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Wwj4xCvZy2",
        "outputId": "5fc8ad19-7f7d-4614-8e61-7ea71bb7af3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "20/20 [==============================] - 72s 3s/step - loss: 3.5641\n",
            "Epoch 2/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 3.2159\n",
            "Epoch 3/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 3.1291\n",
            "Epoch 4/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 2.9656\n",
            "Epoch 5/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 2.7924\n",
            "Epoch 6/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.6316\n",
            "Epoch 7/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.5668\n",
            "Epoch 8/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 2.4550\n",
            "Epoch 9/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.3878\n",
            "Epoch 10/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.3358\n",
            "Epoch 11/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.2680\n",
            "Epoch 12/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.2121\n",
            "Epoch 13/200\n",
            "20/20 [==============================] - 70s 4s/step - loss: 2.1743\n",
            "Epoch 14/200\n",
            "20/20 [==============================] - 69s 3s/step - loss: 2.1349\n",
            "Epoch 15/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.0658\n",
            "Epoch 16/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 2.0390\n",
            "Epoch 17/200\n",
            "20/20 [==============================] - 70s 3s/step - loss: 1.9836\n",
            "Epoch 18/200\n",
            "20/20 [==============================] - 70s 4s/step - loss: 1.9360\n",
            "Epoch 19/200\n",
            "10/20 [==============>...............] - ETA: 35s - loss: 1.8934"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8mzV9U2vpl4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}